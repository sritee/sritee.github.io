---
layout: post
title:  "Human-level control through deep reinforcement learning"
---

[https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf](Human-level control through deep reinforcement learning)


They key contribution of this paper is an RL agent which learns a non-linear feature representation to surpass human performance on 49 atari tasks. Prior work exists using neural network approximators in RL, like TD-gammon but they involved domain dependent features
not general enough to work across a set of tasks.

What does it take to get neural network function approximator to work with Q-learning?

Let's assume the Q-network is parametrized by a set of weights $$w$$.

## Target Network
Let's look at the gradient update for $$w$$ based on a single transition in the environment $$(s, a, r, s')$$

$$mininimize_{w} ( Q_w(s,a) - (r + max_{a'}Q_{w}(s',a'))^2$$

The first term is the predicted value by the Q-network, parametrized by w, and the second term is the actual value based on the 1-step bootstrapped estimate that we wanted it to output.

This term looks very close to the traditional least squares minimization!

But, unlike in supervised learning, the target that we tell the network to output actually depends on the current weights w (it contains $$Q_{w}(s',a')$$). This is almost like a cat and mouse game. As a loose example: we tell the network, hey, output a value 2 for this state-action pair! But once the network is updated, we tell it, hey you should actually output 1.9 for this state! This results in a non-stationary learning problem. 

How can we bring it closer to the supervised learning setting, where the target we tell it output remains fixed, atleast for a period of time? 

The authors proposed to do this by using something called a target network, which is used to generate the target for the weight update. This target network is a copy of the online weight network, but is kept fixed for a constant period of time. This ensures that atleast for this fixed period, the squared error minimization is similar to a supervised learning update and is stable.

## Replay Buffer

The authors propose to maintain a replay buffer, which is a list of all transitions $$(S, A, R, S')$$the agent experienced since it began training. At every training step, a random mini-batch of data is sampled from this buffer, and we minimize the squared bellman error over this mini-batch for this gradient update.

Why?

- **IID assumption**: When minimizing the squared bellman error, we are basically solving a least-squares supervised learning problem, although the target itself depends on the weights. Supervised learning makes an $$IID$$ assumption - identically, independently distributed data. If we use transitions gathered from the most recent episode to do the least squares fitting, they break the 'independent' assumption as the data will be highly correlated.
- **Minibatch local overfitting**-- This point is not made explicitly in the paper, but is my personal interpretation. Since we are interested in learning a global $$Q(s,a)$$, which is good throughout the state space in which the optimal policy visits, doing a gradient update on a minibatch which contains only just the last few episodes might result in overfitting to this small region. 

But hey, doesn't minibatch learning work fine in the supervised learning setting? The problem is much more severe in the reinforcement learning setting, since the network that we update $$Q_w(s,a)$$ actually affects the data that we collect (the exploration policy). Hence an wrong, overfit, local update could cause catastrophic issues in learning.


## Tricks
- **Frame Skipping**: Repeat the action chosen by the agent for 4 frames. This makes the control problem slightly easier, since high frequency control involves a longer effective horizon and is a harder exploration problem.
- **Frame Stacking**: Stack the last 4 Atari frames and feed it to the agent as the current observation. This ensures the agent's observation contains sufficient dynamic information for control. As an example, in the game of pong, velocity cannot be inferred with just the latest frame, and we might need a history. Feeding a single frame results in the agent being in a $$POMDP$$, a partially observable Markov Decision Process.


